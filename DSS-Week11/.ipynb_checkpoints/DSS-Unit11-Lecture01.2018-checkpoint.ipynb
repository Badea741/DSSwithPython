{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introduction to Linear Regression through Scikit-Learn</h1>\n",
    "<h2>What is scikit-learn? </h2>\n",
    "<p style=\"font-size: 16px\">Scikit is a machine learning python library built off of packages you have recently been introduced to such as numpy, scipy and matplotlib. For more information, visit the <a href='http://scikit-learn.org/stable/index.html#'>scikit-learn homepage</a> or refer to the Python Data Science Handbook (VanderPlas, Jake) pages 343-359.<br>\n",
    "\n",
    "<br>The library contains function in the following machine learning categories:\n",
    "<li style=\"font-size: 16px\"><a href='http://scikit-learn.org/stable/supervised_learning.html#supervised-learning'> Classification</a> </li>\n",
    "<li style=\"font-size: 16px\"><a href='http://scikit-learn.org/stable/supervised_learning.html#supervised-learning'> Regression </a></li>\n",
    "<li style=\"font-size: 16px\"><a href='http://scikit-learn.org/stable/modules/clustering.html#clustering'> Clustering </a></li>\n",
    "<li style=\"font-size: 16px\"><a href='http://scikit-learn.org/stable/modules/decomposition.html#decompositions'>Dimensionality Reduction</a></li>\n",
    "<li style=\"font-size: 16px\"><a href='http://scikit-learn.org/stable/model_selection.html#model-selection'>Model Selection</a></li>\n",
    "<li style=\"font-size: 16px\"><a href='http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing'>Preprocessing </a></li>\n",
    "\n",
    "<p style=\"font-size: 16px\">Scikit-learn should be installed along with your Anaconda3 installation. However, if this is not the case, follow the installation instructions provided by scikit-learn <a href='http://scikit-learn.org/stable/install.html'>here</a><br>\n",
    "\n",
    "<br>Next, lets import some of the packages we will use and see what version you are running!</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 0.19.1.\n",
      "The pandas version is 0.23.0\n",
      "The matplotlib version is 2.2.2\n",
      "The numpy version is 1.14.3\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))\n",
    "import pandas as pd\n",
    "print('The pandas version is {}'.format(pd.__version__))\n",
    "import matplotlib\n",
    "print('The matplotlib version is {}'.format(matplotlib.__version__))\n",
    "import numpy as np\n",
    "print('The numpy version is {}'.format(np.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>What is Linear Regression?</h2>\n",
    "<p style=\"font-size: 16px\">Linear regression is a type of linear model used to express the relation between the response $y\\in\\mathbb{R}^n$ and a combination of one ore more independent variables $x_i\\in\\mathbb{R}^n$. The response $y$ depends linearly on the $d$ variables $x_i$.\n",
    "\n",
    "$$Y_i = a_0 + a_1x_1+...+a_dx_d+\\epsilon_i$$\n",
    "\n",
    "<li style=\"font-size: 16px\">$Y_i$ is the predicted value</li>\n",
    "<li style=\"font-size: 16px\">$a_0$ is the y-intercept or constant term</li>\n",
    "<li style=\"font-size: 16px\">$a_d$ are the <i>parameters</i> or <i>coefficients</i> of the model</li>\n",
    "<li style=\"font-size: 16px\">$x_d$ is the independent variable</li>\n",
    "<li style=\"font-size: 16px\">$\\epsilon_i$ is the random error</li>\n",
    "\n",
    "<br><p style=\"font-size: 16px\">The matrix version of the equation is: $\\mathbf{y}=\\mathbf{X}\\mathbf{w}$ where</p>\n",
    "<br>\n",
    "$$y=  \\begin{pmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n \\end{pmatrix}, \n",
    "X = \\begin{pmatrix} x_{1,1} & \\cdots & x_{1,d} \\\\ x_{2,1} & \\cdots & x_{2,d} \\\\\n",
    "& \\vdots & \\\\x_{n,1} & \\cdots & x_{n,d} \\end{pmatrix}, \n",
    "w = \\begin{pmatrix} a_1 \\\\ a_2 \\\\ \\vdots \\\\ a_n \\end{pmatrix}$$\n",
    "\n",
    "<p style=\"font-size: 16px\">\n",
    " These coefficients are learned by minimizing the squared difference between the predicted values and the actual values, this is called the <i>sum of squared errors of prediction (SSE)</i>. This minimization is called the <i> ordinary least squares (OLS) estimator</i>.\n",
    "$$\\Vert a_0+a_1\\mathbf{x}-\\mathbf{y}\\Vert_{2}^2=\\sum_{j=1}^n(a_0+a_1x_j-y_j)^2$$\n",
    "\n",
    "\n",
    "<p style=\"font-size: 16px\">The SSE posses a unique global minimum, which is what we fit to.</p>\n",
    "<p style=\"font-size: 16px\">For more information, refer to Python Data Science Handbook pages 331-381 and pages 390-396."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Introduction to Data Science Ch06 Case 1: Sea Ice Data and Climate Change</h2>\n",
    "<p style=\"font-size: 16px\">This case scenario is interested in showing the effect of climate change by determining whether the sea ice area (or extent) has decreased over the years. Sea ice extent refers to the area of ocean with at least 15% sea ice while sea ice area refers to the total area covered by ice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File files_ch06/SeaIce.txt does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-6c6aba6cf239>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'files_ch06/SeaIce.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelim_whitespace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'shape: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mice\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ra407452\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ra407452\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ra407452\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ra407452\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\ra407452\\AppData\\Local\\Continuum\\anaconda2\\lib\\site-packages\\pandas\\io\\parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mIOError\u001b[0m: File files_ch06/SeaIce.txt does not exist"
     ]
    }
   ],
   "source": [
    "ice = pd.read_csv('files_ch06/SeaIce.txt', delim_whitespace=True)\n",
    "print('shape: {}'.format(ice.shape))\n",
    "ice.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 16px\"><b>Data Preperation</b></p>\n",
    "<p style=\"font-size: 16px\">\n",
    "Before we begin fitting a model, we will run a short analysis on the dataset. We will first start with visualizing the variables and the relationship they have with eachother and the target variable. We will start with importing matplotlib's pyplot package for generating plots and format the figure size for inline visualization. We will also utilize the Seaborn package for visualization. Seaborn is a higher level package that uses matplotlib to create more statistical based visualizations and is better integrated with Pandas. Some features include:<br> <br>\n",
    "\n",
    "<li style=\"font-size: 16px\">Several built-in themes for styling matplotlib graphics</li>\n",
    "<li style=\"font-size: 16px\">Tools for choosing color palettes to make beautiful plots that reveal patterns in your data</li>\n",
    "<li style=\"font-size: 16px\">Functions for visualizing univariate and bivariate distributions or for comparing them between subsets of data</li>\n",
    "<li style=\"font-size: 16px\">Tools that fit and visualize linear regression models for different kinds of independent and dependent variables</li>\n",
    "<li style=\"font-size: 16px\">Functions that visualize matrices of data and use clustering algorithms to discover structure in those matrices</li>\n",
    "<li style=\"font-size: 16px\">A function to plot statistical timeseries data with flexible estimation and representation of uncertainty around the estimate</li>\n",
    "<li style=\"font-size: 16px\">High-level abstractions for structuring grids of plots that let you easily build complex visualizations</li>\n",
    "<p style=\"font-size: 16px\"><a href=\"https://seaborn.pydata.org/introduction.html#introduction\">Click for more about seaborn</a>  or refer to Python Data Science Handbook (VanderPlas, Jake) pages 311-313.<br><br>\n",
    "First we will describe the data using pandas so we can see if there are any issues with the data. See if you notice anything odd in the table below.  </p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "print(\"Variables: {}\".format(ice.columns.values))\n",
    "ice.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 16px\"> As you can see from the above table, there are negative means and negative large numbers for the minimum values for both the extent and area, indicating some data preperation needs to occur before we can build our model.  Through visualization we can see the same issue. For more on matplotlib subplots, refer to Python Data Science Handbook (VanderPlas, Jake) pages 262-266."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(10,8))\n",
    "ice.plot.scatter(x='year', y='area', legend=False, sharex=False, sharey=True, ax=ax1, grid=True);\n",
    "ice.plot.scatter(x='year', y='extent', legend=False, ax=ax3, grid=True)\n",
    "ice.plot.scatter(x='mo', y='area',  legend=False, ax=ax2, grid=True)\n",
    "ice.plot.scatter(x='mo', y='extent', legend=False, ax=ax4, grid=True)\n",
    "fig.autofmt_xdate()\n",
    "ax1.set_title('by Year')\n",
    "ax2.set_title('by Month')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "ice[['extent', 'area']].plot.box(ax=axes[0])\n",
    "ice.query('extent > 0 and area > 0')[['extent', 'area']].plot.box(ax=axes[1]) # this is removing the extreme values\n",
    "axes[0].set_title('With Raw Data')\n",
    "plot_ = axes[1].set_title('Without Extreme Values')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 16px\">From the above, we can see that if we removed the extreme values, we would havea fairly normal distribution for both variables. Now that we have investigated continuous variables, let us take a look at our categorical variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ice['data_type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ice[(ice.data_type != 'Goddard') & (ice.data_type != 'NRTSI-G')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 16px\">As can be seen, there are two extreme values that indicate missing values. Let's remove them and continue with our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ice_cleaned = ice[ice['data_type'] != '-9999']\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(10,8))\n",
    "ice_cleaned.plot.scatter(x='year', y='area', legend=False, sharex=False, sharey=True, ax=ax1, grid=True);\n",
    "ice_cleaned.plot.scatter(x='year', y='extent', legend=False, ax=ax3, grid=True)\n",
    "ice_cleaned.plot.scatter(x='mo', y='area',  legend=False, ax=ax2, grid=True)\n",
    "ice_cleaned.plot.scatter(x='mo', y='extent', legend=False, ax=ax4, grid=True)\n",
    "fig.autofmt_xdate()\n",
    "ax1.set_title('by Year')\n",
    "ax2.set_title('by Month')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 16px\">From the plots you can see that by removing the extreme values from our categorical variable, it also removed them from our other two variables 'area' and 'extent'. We can now proceed further with our analysis by visualizing potential linear relationships. For more on using seaborn, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "sns.regplot(x=ice_cleaned['mo'], y=ice_cleaned['extent'], ax=ax1)\n",
    "ax1.grid(True)\n",
    "plot_ = sns.regplot(x=ice_cleaned['mo'], y=ice_cleaned['area'], ax=ax2)\n",
    "ax2.grid(True)\n",
    "\n",
    "# the equivalent using lmplot() would be:\n",
    "#sns.lmplot(x='mo', y='extent', data=ice)\n",
    "# sns.lmplot(x='mo', y='area', data=ice);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 16px\">From the plots we observe fluctuations over time, indicating we should normalize the data: \n",
    "$$e_j^{-i}=100*\\frac{e_j^{-i}-\\mu_i}{\\mu}, i=1,...,12.$$\n",
    "<li style=\"font-size: 16px\">$u_i$ mean of the i-th interval of time (month)</li>\n",
    "<li style=\"font-size: 16px\">$e_j^i$ set of independent variable values for that month</li>\n",
    "<li style=\"font-size: 16px\">$\\mu$ total mean, used to convert the final value to a relative percent difference</li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grouped = ice_cleaned.groupby('mo')  # group by first so do not have to run multiple group by computations\n",
    "month_means = grouped.extent.mean()\n",
    "month_mean = month_means.mean()\n",
    "ice_cleaned = ice_cleaned.assign(n_extent = grouped.extent.apply(lambda x: 100*((x-x.mean())/month_mean)))\n",
    "month_means = grouped.area.mean()\n",
    "month_mean = month_means.mean()\n",
    "ice_cleaned = ice_cleaned.assign(n_area = grouped.area.apply(lambda x: 100*((x-x.mean())/month_mean)))\n",
    "ice_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "sns.regplot(x=ice_cleaned['mo'], y=ice_cleaned['n_extent'], ax=ax1)\n",
    "ax1.grid(True)\n",
    "sns.regplot(x=ice_cleaned['mo'], y=ice_cleaned['n_area'], ax=ax2)\n",
    "ax2.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 16px\">From the plots you see that we successfully normalized the values in accordance to the monthly fluctuations. Let's visualize this now over the course of years.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2)\n",
    "sns.regplot(x=ice_cleaned['year'], y=ice_cleaned['n_extent'], ax=ax1)\n",
    "ax1.grid(True)\n",
    "sns.regplot(x=ice_cleaned['year'], y=ice_cleaned['n_area'], ax=ax2)\n",
    "ax2.grid(True)\n",
    "ax1.set_title('Extent')\n",
    "ax2.set_title('Area')\n",
    "fig.autofmt_xdate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 16px\">From the plots we can see a downward trend that could be attributed to global warming, but the variation over the years should be taken into consideration.</p>\n",
    "\n",
    "<p style=\"font-size: 16px\">Next we can look at the at the Pearson correlation between the variables and the p-values for testing for non-correlation.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ice_cleaned.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 16px\">The F-score and p-value are metrics used to provide support to either the null hypothesis or the alternative hypothesis. The null hypothesis states that a model with no predictor variables will represent our data as well as our model did, i.e. an intercept only model. The alternative hypothesis is that our model predicts better than an intercept only model. From the F-test we get the p-value which indicates the significance of a variable. If our variable is above our threshold which is normally 0.05, than the variable significantly improved our model. This will help us pick variables that are meaninful in our model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression as f_test\n",
    "F_score_extent, p_value_extent = f_test(ice_cleaned[['year']], ice_cleaned['n_extent'])\n",
    "F_score_area, p_value_area = f_test(ice_cleaned[['year']], ice_cleaned['n_area'])\n",
    "pd.DataFrame(data={'extent~year':{'p_value':p_value_extent[0], 'F_score': F_score_extent[0]},\n",
    "                  'area~year':{'p_value':p_value_area[0], 'F_score': F_score_area[0]}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 16px\">According to the F-test, there is a significant relationship between year and extent.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Fitting the Model</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 16px\">To fit the model, we will use sklearn's object oriented interface. Firstly we create an object, which we name 'model'. We then can use the model.fit method to set the state of the object based on the training data. The data passed to the method must be in a two dimensional numpy array $\\mathbf{X}$ of shape(n_samples, n_predictors holding the feature matrix and a one-dimensinal numpy array $\\mathbf{y}$ that holds the response variable values. To view the documentaiton on this method, <a href='http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html'>visit here</a>.</p>\n",
    "\n",
    "<p style=\"font-size: 16px\">Once you have fit the model using the appropriate parameters passed to the fit method, the new state of the model object is stored in instances attributes with a trailing underscore '_' (i.e. model.coefficients_).</p>\n",
    "\n",
    "<p style=\"font-size: 16px\">Estimator objects that can generate predictions provide a model.predict method. In the case of regression, model.predict will return the predicted regression values, $\\hat{\\mathbf{y}}$.\n",
    "\n",
    "<p style=\"font-size: 16px\">We will start with fitting extent onto year. Using a subset of the data to train and a subset to test the model.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = ice_cleaned[['year']]\n",
    "y = ice_cleaned[['n_extent']]\n",
    "\n",
    "# Create linear regression object\n",
    "model = linear_model.LinearRegression()\n",
    "\n",
    "# Train the model using the training sets\n",
    "model.fit(X, y)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "# Plot outputs\n",
    "plt.figure(figsize=(14,12))\n",
    "plt.plot(X, y,'o', alpha = 0.5)\n",
    "plt.plot(X, y_pred, 'r', alpha = 0.5)\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('extent (All months)')\n",
    "plt.plot(X, y_pred, linewidth=3)\n",
    "plt.grid(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
